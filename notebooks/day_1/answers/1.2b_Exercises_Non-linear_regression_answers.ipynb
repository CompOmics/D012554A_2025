{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde8e984",
   "metadata": {},
   "source": [
    "# 1.2 Exercises – Non-linear Regression (Answers)\n",
    "\n",
    "In this notebook you will practice the non-linear regression techniques from the lecture using a **new dataset**.\n",
    "\n",
    "## Dataset: Diamond Prices\n",
    "\n",
    "We will use the [diamonds dataset](https://ggplot2.tidyverse.org/reference/diamonds.html), which contains prices and attributes of ~54,000 diamonds. For efficiency we work with a random sample of **500 diamonds**.\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| `carat` | Weight of the diamond (0.2–5.01) |\n",
    "| `price` | Price in US dollars ($326–$18,823) |\n",
    "\n",
    "The relationship between `carat` and `price` is clearly **non-linear** — larger diamonds are disproportionately more expensive.\n",
    "\n",
    "**Goal**: Use polynomial regression and other non-linear techniques to model diamond prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08603830",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b2b0e",
   "metadata": {},
   "source": [
    "## Exercise 1 – Load and explore the dataset\n",
    "\n",
    "Load the diamonds dataset from the URL below and take a random sample of 500 rows (use `random_state=42`). Keep only the `carat` and `price` columns.\n",
    "\n",
    "Display the shape, basic statistics (`.describe()`), and the first 5 rows.\n",
    "\n",
    "```python\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/diamonds.csv\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/diamonds.csv\"\n",
    "\n",
    "diamonds = pd.read_csv(url)\n",
    "dataset = diamonds[['carat', 'price']].sample(n=500, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {dataset.shape}\")\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(dataset.describe())\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3dc0a9",
   "metadata": {},
   "source": [
    "## Exercise 2 – Visualize the relationship\n",
    "\n",
    "Create a scatter plot of `carat` (x-axis) versus `price` (y-axis) using `sns.lmplot` with `fit_reg=False`.\n",
    "\n",
    "Does the relationship look linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf018b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"carat\", y=\"price\", data=dataset,\n",
    "           fit_reg=False, height=7, scatter_kws={\"s\": 40, \"alpha\": 0.6})\n",
    "plt.title(\"Diamond Price vs. Carat\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c913dc",
   "metadata": {},
   "source": [
    "## Exercise 3 – Fit a linear model\n",
    "\n",
    "Fit a `LinearRegression` model using `carat` as the single feature to predict `price`.\n",
    "\n",
    "1. Compute and print the R² score.\n",
    "2. Plot the data points and overlay the linear fit line.\n",
    "\n",
    "**Hint**: Use `np.linspace` to generate evenly spaced x-values for plotting the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5650bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = dataset[['carat']].values\n",
    "y = dataset['price'].values\n",
    "\n",
    "model_linear = LinearRegression(fit_intercept=True)\n",
    "model_linear.fit(X, y)\n",
    "y_pred_linear = model_linear.predict(X)\n",
    "\n",
    "print(f\"R-squared (linear) = {r2_score(y, y_pred_linear):.4f}\")\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(dataset['carat'], y, s=40, alpha=0.6, label='Data')\n",
    "x_plot = np.linspace(dataset['carat'].min(), dataset['carat'].max(), 100)\n",
    "plt.plot(x_plot, model_linear.predict(x_plot.reshape(-1, 1)),\n",
    "         'r-', linewidth=2, label='Linear fit')\n",
    "plt.xlabel('Carat')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Linear Model Fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21edfe36",
   "metadata": {},
   "source": [
    "## Exercise 4 – Evaluate the linear model\n",
    "\n",
    "Is the linear model a good fit for this data? Why or why not? Write your answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ff8a5",
   "metadata": {},
   "source": [
    "The linear model captures the general upward trend but is **not a good fit**. The scatter plot shows a clear **curved / non-linear** relationship: price increases slowly for small carats but accelerates for larger carats. The linear model under-predicts for very small and very large diamonds and over-predicts in the middle range. The R², while reasonable, can be improved significantly with non-linear approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5888191",
   "metadata": {},
   "source": [
    "## Exercise 5 – Polynomial model (degree 2)\n",
    "\n",
    "Add a new feature `carat^2` (the square of `carat`) to the dataset. Fit a `LinearRegression` model using both `carat` and `carat^2` as features.\n",
    "\n",
    "1. Print the R² score and compare it to the linear model.\n",
    "2. Plot the data and overlay the degree-2 polynomial curve.\n",
    "\n",
    "**Hint**: To plot the curve, compute predictions as:\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 \\cdot x + \\theta_2 \\cdot x^2$$\n",
    "\n",
    "where the coefficients come from `model.intercept_` and `model.coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5bacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['carat^2'] = dataset['carat'] ** 2\n",
    "\n",
    "X_poly2 = dataset[['carat', 'carat^2']].values\n",
    "\n",
    "model_poly2 = LinearRegression(fit_intercept=True)\n",
    "model_poly2.fit(X_poly2, y)\n",
    "y_pred_poly2 = model_poly2.predict(X_poly2)\n",
    "\n",
    "print(f\"R-squared (degree 2) = {r2_score(y, y_pred_poly2):.4f}\")\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(dataset['carat'], y, s=40, alpha=0.6, label='Data')\n",
    "x_plot = np.linspace(dataset['carat'].min(), dataset['carat'].max(), 100)\n",
    "pred_plot = (model_poly2.intercept_\n",
    "             + model_poly2.coef_[0] * x_plot\n",
    "             + model_poly2.coef_[1] * x_plot**2)\n",
    "plt.plot(x_plot, pred_plot, 'r-', linewidth=2, label='Degree 2')\n",
    "plt.xlabel('Carat')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Polynomial Model (Degree 2)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460f8ac",
   "metadata": {},
   "source": [
    "## Exercise 6 – Higher-degree polynomials with scaling\n",
    "\n",
    "Fit polynomial models of degrees 2 through 7 and plot all fits on a single figure.\n",
    "\n",
    "**Important**: Higher-degree polynomial features (e.g. `carat^7`) can have very large values. Use `MinMaxScaler` to scale each polynomial feature to the [0, 1] range before fitting.\n",
    "\n",
    "**Steps**:\n",
    "1. Loop over degrees 2 to 7.\n",
    "2. For each degree, build a feature matrix with columns `carat`, `carat^2`, ..., `carat^d`.\n",
    "3. Scale the polynomial features (degree ≥ 2) using `MinMaxScaler`.\n",
    "4. Fit a `LinearRegression` and store the R² score.\n",
    "5. Generate predictions for plotting (remember to scale the plot features with the **same** scaler).\n",
    "6. Show all polynomial fits on one scatter plot with a legend that includes the R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d76eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(dataset['carat'], y, s=40, alpha=0.4, label='Data', color='grey')\n",
    "\n",
    "x_plot = np.linspace(dataset['carat'].min(), dataset['carat'].max(), 200)\n",
    "\n",
    "for degree in [2, 3, 4, 5, 6, 7]:\n",
    "    # Build feature matrices\n",
    "    X_train = pd.DataFrame({'carat': dataset['carat']})\n",
    "    X_pred = pd.DataFrame({'carat': x_plot})\n",
    "\n",
    "    for d in range(2, degree + 1):\n",
    "        X_train[f'carat^{d}'] = dataset['carat'] ** d\n",
    "        X_pred[f'carat^{d}'] = x_plot ** d\n",
    "\n",
    "    # Scale polynomial features\n",
    "    poly_cols = [f'carat^{d}' for d in range(2, degree + 1)]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train[poly_cols] = scaler.fit_transform(X_train[poly_cols])\n",
    "    X_pred[poly_cols] = scaler.transform(X_pred[poly_cols])\n",
    "\n",
    "    # Fit and predict\n",
    "    model = LinearRegression(fit_intercept=True)\n",
    "    model.fit(X_train, y)\n",
    "    r2 = r2_score(y, model.predict(X_train))\n",
    "\n",
    "    pred = model.predict(X_pred)\n",
    "    plt.plot(x_plot, pred, linewidth=2,\n",
    "             label=f'Degree {degree} (R\\u00b2={r2:.3f})')\n",
    "\n",
    "plt.xlabel('Carat')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Polynomial Fits of Different Degrees')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10244513",
   "metadata": {},
   "source": [
    "## Exercise 7 – Train/test split: detecting overfitting\n",
    "\n",
    "Split the data into 80% training and 20% test sets (use `random_state=42`). For each polynomial degree from 2 to 9:\n",
    "\n",
    "1. Build polynomial features and scale them with `MinMaxScaler`.\n",
    "2. Fit on the **training** set.\n",
    "3. Compute R² on both the training and test sets.\n",
    "\n",
    "Print a table showing degree, training R², and test R².\n",
    "\n",
    "**Hint**: Use `train_test_split` from `sklearn.model_selection`. Make sure to `fit` the scaler on the training data only and `transform` both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fccf3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_all = dataset['carat'].values\n",
    "y_all = dataset['price'].values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"{'Degree':<8} {'Train R\\u00b2':<12} {'Test R\\u00b2':<12}\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "for degree in range(2, 10):\n",
    "    # Build polynomial features\n",
    "    X_train_df = pd.DataFrame({'carat': X_tr})\n",
    "    X_test_df = pd.DataFrame({'carat': X_te})\n",
    "\n",
    "    for d in range(2, degree + 1):\n",
    "        X_train_df[f'carat^{d}'] = X_tr ** d\n",
    "        X_test_df[f'carat^{d}'] = X_te ** d\n",
    "\n",
    "    # Scale polynomial features (fit on train only)\n",
    "    poly_cols = [f'carat^{d}' for d in range(2, degree + 1)]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_df[poly_cols] = scaler.fit_transform(X_train_df[poly_cols])\n",
    "    X_test_df[poly_cols] = scaler.transform(X_test_df[poly_cols])\n",
    "\n",
    "    # Fit and evaluate\n",
    "    model = LinearRegression(fit_intercept=True)\n",
    "    model.fit(X_train_df, y_tr)\n",
    "\n",
    "    r2_train = r2_score(y_tr, model.predict(X_train_df))\n",
    "    r2_test = r2_score(y_te, model.predict(X_test_df))\n",
    "\n",
    "    print(f\"{degree:<8} {r2_train:<12.4f} {r2_test:<12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d488ab",
   "metadata": {},
   "source": [
    "## Exercise 8 – Interpret the results\n",
    "\n",
    "Based on the training vs. test R² values:\n",
    "- Which polynomial degree gives the best **test** performance?\n",
    "- At what degree do you start to see signs of **overfitting** (training R² much higher than test R²)?\n",
    "\n",
    "Write your answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ba593d",
   "metadata": {},
   "source": [
    "A relatively low degree (around 3–4) typically gives the best test R². As the degree increases, the training R² keeps rising (the model memorizes the training data better) but the test R² starts declining or becomes unstable. The growing gap between training and test R² is the hallmark of **overfitting**. This usually becomes visible around degree 6–7 and higher, where the model fits noise in the training data rather than the true underlying pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d5b7b",
   "metadata": {},
   "source": [
    "## Exercise 9 – Using `PolynomialFeatures`\n",
    "\n",
    "Instead of manually creating polynomial features, use `sklearn.preprocessing.PolynomialFeatures` to automatically generate them.\n",
    "\n",
    "1. Create degree-3 polynomial features from `carat` (set `include_bias=False`).\n",
    "2. Fit a `LinearRegression` model on these features.\n",
    "3. Print the R² score.\n",
    "4. Compare this result with your manual degree-3 polynomial from Exercise 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e949e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly3 = poly.fit_transform(dataset[['carat']])\n",
    "\n",
    "print(f\"Feature names: {poly.get_feature_names_out()}\")\n",
    "print(f\"Shape of transformed features: {X_poly3.shape}\")\n",
    "\n",
    "model_poly3 = LinearRegression(fit_intercept=True)\n",
    "model_poly3.fit(X_poly3, y)\n",
    "\n",
    "r2_poly3 = r2_score(y, model_poly3.predict(X_poly3))\n",
    "print(f\"\\nR-squared (PolynomialFeatures, degree 3) = {r2_poly3:.4f}\")\n",
    "print(\"\\nThis should match the manual degree-3 result from Exercise 6.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b370f4c",
   "metadata": {},
   "source": [
    "## Exercise 10 – Sigmoid curve fitting\n",
    "\n",
    "As discussed in the lecture, many biological relationships follow a **sigmoid (logistic) function**:\n",
    "\n",
    "$$f(x) = \\frac{L}{1 + e^{-\\theta_1(x - \\theta_0)}}$$\n",
    "\n",
    "where:\n",
    "- $L$ is the curve's maximum value\n",
    "- $\\theta_0$ is the x-value of the sigmoid's midpoint\n",
    "- $\\theta_1$ controls the steepness\n",
    "\n",
    "The cell below provides synthetic dose-response data. Your task:\n",
    "\n",
    "1. Define a Python function `sigmoid(x, L, theta0, theta1)` implementing the formula above.\n",
    "2. Fit the parameters using `scipy.optimize.curve_fit`.\n",
    "3. Plot the data and the fitted sigmoid curve.\n",
    "4. Print the fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic dose-response data (provided)\n",
    "np.random.seed(42)\n",
    "dose = np.linspace(0, 10, 30)\n",
    "response = 100 / (1 + np.exp(-(1.5 * (dose - 5)))) + np.random.normal(0, 3, size=30)\n",
    "response = np.clip(response, 0, 100)\n",
    "dose_response = pd.DataFrame({'dose': dose, 'response': response})\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(dose_response['dose'], dose_response['response'], s=60)\n",
    "plt.xlabel('Dose')\n",
    "plt.ylabel('Response (%)')\n",
    "plt.title('Dose-Response Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b45990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def sigmoid(x, L, theta0, theta1):\n",
    "    \"\"\"Sigmoid (logistic) function.\"\"\"\n",
    "    return L / (1 + np.exp(-theta1 * (x - theta0)))\n",
    "\n",
    "# Fit the sigmoid to the dose-response data\n",
    "popt, pcov = curve_fit(\n",
    "    sigmoid,\n",
    "    dose_response['dose'],\n",
    "    dose_response['response'],\n",
    "    p0=[100, 5, 1]  # initial guesses: L=100, theta0=5, theta1=1\n",
    ")\n",
    "\n",
    "L_fit, theta0_fit, theta1_fit = popt\n",
    "print(f\"Fitted parameters:\")\n",
    "print(f\"  L      = {L_fit:.2f}  (maximum response)\")\n",
    "print(f\"  theta0 = {theta0_fit:.2f}  (midpoint)\")\n",
    "print(f\"  theta1 = {theta1_fit:.2f}  (steepness)\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(dose_response['dose'], dose_response['response'],\n",
    "            s=60, label='Data', zorder=5)\n",
    "x_smooth = np.linspace(0, 10, 200)\n",
    "plt.plot(x_smooth, sigmoid(x_smooth, *popt),\n",
    "         'r-', linewidth=2, label=f'Sigmoid fit')\n",
    "plt.xlabel('Dose')\n",
    "plt.ylabel('Response (%)')\n",
    "plt.title('Dose-Response Curve (Sigmoid Fit)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc112e9e",
   "metadata": {},
   "source": [
    "## Bonus – Log-transform the target variable\n",
    "\n",
    "Sometimes a non-linear relationship can be made approximately linear by **transforming the target variable**.\n",
    "\n",
    "1. Create `log_price = log(price)` using `np.log`.\n",
    "2. Fit a simple `LinearRegression` model: `carat` → `log_price`.\n",
    "3. Compute R² and plot the fit.\n",
    "4. Compare this R² to your polynomial models.\n",
    "\n",
    "**Why does this work?** If price ≈ $e^{a + b \\cdot \\text{carat}}$, then $\\log(\\text{price}) \\approx a + b \\cdot \\text{carat}$, which is linear!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5380aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform the price\n",
    "dataset['log_price'] = np.log(dataset['price'])\n",
    "\n",
    "X_carat = dataset[['carat']].values\n",
    "y_log = dataset['log_price'].values\n",
    "\n",
    "model_log = LinearRegression(fit_intercept=True)\n",
    "model_log.fit(X_carat, y_log)\n",
    "\n",
    "r2_log = r2_score(y_log, model_log.predict(X_carat))\n",
    "print(f\"R-squared (log-price, linear model) = {r2_log:.4f}\")\n",
    "print(f\"\\nCompare with linear model on raw price: {r2_score(y, y_pred_linear):.4f}\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: log-transformed fit\n",
    "axes[0].scatter(dataset['carat'], y_log, s=40, alpha=0.6, label='Data')\n",
    "x_plot = np.linspace(dataset['carat'].min(), dataset['carat'].max(), 100)\n",
    "axes[0].plot(x_plot, model_log.predict(x_plot.reshape(-1, 1)),\n",
    "             'r-', linewidth=2, label='Linear fit')\n",
    "axes[0].set_xlabel('Carat')\n",
    "axes[0].set_ylabel('log(Price)')\n",
    "axes[0].set_title(f'Linear fit on log(Price) \\u2014 R\\u00b2={r2_log:.4f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: back-transformed prediction on original scale\n",
    "axes[1].scatter(dataset['carat'], y, s=40, alpha=0.6, label='Data')\n",
    "y_plot_log = model_log.predict(x_plot.reshape(-1, 1))\n",
    "axes[1].plot(x_plot, np.exp(y_plot_log),\n",
    "             'r-', linewidth=2, label='exp(Linear fit)')\n",
    "axes[1].set_xlabel('Carat')\n",
    "axes[1].set_ylabel('Price ($)')\n",
    "axes[1].set_title('Back-transformed to original scale')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
