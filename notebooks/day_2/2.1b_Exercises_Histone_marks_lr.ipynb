{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44493094",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CompOmics/D012554A_2025/blob/main/notebooks/day_2/2.1b_Exercises_Histone_marks_lr.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a22014",
   "metadata": {},
   "source": [
    "# 2.1 Exercises – Logistic Regression & Classification\n",
    "\n",
    "In the lecture notebook you applied logistic regression to classify gene expression levels from histone modification signals. In these exercises you will apply those same techniques to a new binary classification problem: diagnosing breast tumours as malignant or benign.\n",
    "\n",
    "## Dataset: Breast Cancer Wisconsin (Diagnostic)\n",
    "\n",
    "The dataset is computed from digitized images of fine needle aspirate (FNA) of breast masses. It contains 30 real-valued features describing cell nuclei:\n",
    "\n",
    "| Feature group | Examples |\n",
    "|---------------|----------|\n",
    "| Mean values | `mean radius`, `mean texture`, `mean perimeter`, ... |\n",
    "| Standard errors | `radius error`, `texture error`, ... |\n",
    "| Worst (largest) values | `worst radius`, `worst texture`, ... |\n",
    "\n",
    "The target is the diagnosis: `0 = malignant`, `1 = benign`.\n",
    "\n",
    "Throughout these exercises you will:\n",
    "1. Load and explore the data\n",
    "2. Fit a baseline logistic regression model\n",
    "3. Evaluate with accuracy and log-loss\n",
    "4. Apply feature scaling and re-evaluate\n",
    "5. Tune the regularisation hyperparameter `C` with cross-validation\n",
    "6. Inspect feature importance from the model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac09cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "random_seed = 123\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8ffa4",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1 – Load and explore the dataset\n",
    "\n",
    "Load the Breast Cancer dataset using `sklearn.datasets.load_breast_cancer()` and convert it to a Pandas DataFrame.\n",
    "\n",
    "1. Print the shape of the feature matrix.\n",
    "2. Display the first 5 rows.\n",
    "3. Show the class distribution (how many malignant vs. benign?).\n",
    "\n",
    "Hint: The returned object has `.data`, `.feature_names`, and `.target` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183428c3",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2 – Visualize the features\n",
    "\n",
    "1. Create a heatmap of a random sample of 20 rows (use `sns.heatmap`).\n",
    "2. Create boxplots of all 30 features.\n",
    "\n",
    "What do you notice about the scales of different features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a971a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d319bf",
   "metadata": {},
   "source": [
    "*Your observations here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e0febc",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3 – Boxplots grouped by feature type\n",
    "\n",
    "The 30 features fall into three groups of 10 based on their suffix:\n",
    "- mean features (e.g. `mean radius`)\n",
    "- error features (e.g. `radius error`)\n",
    "- worst features (e.g. `worst radius`)\n",
    "\n",
    "Create separate boxplots for each group (3 plots total), similar to the lecture’s per-histone-mark boxplots.\n",
    "\n",
    "Hint: Use a list comprehension to select columns containing `\"mean\"`, `\"error\"`, or `\"worst\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e775ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba2b16",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4 – Train/validation split & first logistic regression\n",
    "\n",
    "1. Split the data into 80% training / 20% validation (`random_state=123`).\n",
    "2. Fit a `LogisticRegression` model (set `max_iter=10000`).\n",
    "3. Compute and print the accuracy on both the training and validation sets.\n",
    "\n",
    "Hint: Use `train_test_split` from `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4db19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb39dd",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 5 – Evaluate with log-loss\n",
    "\n",
    "Accuracy alone can be misleading. Compute the log-loss on both the training and validation sets using `predict_proba`.\n",
    "\n",
    "The log-loss formula is:\n",
    "\n",
    "$$-\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log p_i + (1-y_i) \\log (1-p_i) \\right]$$\n",
    "\n",
    "Print both the training and validation log-loss.\n",
    "\n",
    "Hint: Use `log_loss` from `sklearn.metrics` and the `[:,1]` column of `predict_proba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c6308",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 6 – Feature scaling with MinMaxScaler\n",
    "\n",
    "As you saw in the boxplots, the features are at very different scales.\n",
    "\n",
    "1. Scale all features to [0, 1] using `MinMaxScaler`.\n",
    "2. Important: Fit the scaler on the training set only, then transform both train and validation.\n",
    "3. Fit a new `LogisticRegression` on the scaled data.\n",
    "4. Print the accuracy and log-loss on both sets.\n",
    "5. Compare with the unscaled results from Exercises 4–5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a186b3f",
   "metadata": {},
   "source": [
    "*Your comparison here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c3802f",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 7 – Hyperparameter tuning with GridSearchCV\n",
    "\n",
    "The regularisation parameter `C` controls how much the model penalises large coefficients.\n",
    "\n",
    "1. Define a parameter grid: `C = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 10, 100]`.\n",
    "2. Use `GridSearchCV` with 5-fold cross-validation and `scoring='neg_log_loss'`.\n",
    "3. Fit on the scaled training data.\n",
    "4. Print the best `C` value.\n",
    "5. Plot the mean cross-validation log-loss vs. `C` (use a log scale for the x-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5378a2c",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 8 – Evaluate the tuned model\n",
    "\n",
    "Using the `best_estimator_` from the grid search:\n",
    "\n",
    "1. Compute the validation log-loss.\n",
    "2. Compute the validation accuracy.\n",
    "3. Compare with the untuned results from Exercise 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ecb269",
   "metadata": {},
   "source": [
    "*Your comparison here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac882f04",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 9 – Feature importance\n",
    "\n",
    "Logistic regression coefficients can be directly interpreted as feature importances.\n",
    "\n",
    "1. Extract the coefficients from the best model.\n",
    "2. Create a DataFrame with columns `Feature` and `Coefficient`.\n",
    "3. Sort by the absolute value and show the top 10.\n",
    "4. Create a horizontal bar plot of these top 10 features.\n",
    "\n",
    "Which features are the most important for the diagnosis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e9463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955106b",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus – Confusion matrix & precision/recall\n",
    "\n",
    "In medical diagnosis, false negatives (missing a malignant tumour) can be more dangerous than false positives.\n",
    "\n",
    "1. Compute the confusion matrix on the validation set using `sklearn.metrics.confusion_matrix`.\n",
    "2. Visualise it as a heatmap with `sns.heatmap(annot=True)`.\n",
    "3. Print the precision and recall for the malignant class (class 0).\n",
    "\n",
    "Hint: Use `classification_report` from sklearn for a quick summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee04499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
